{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning using Cascaded Training\n",
        "\n",
        "Due to limited computational resources, the fine-tuning of the mBART model was conducted using a **cascaded training** approach ‚Äî splitting the process into sequential, resource-efficient stages. This allowed us to gradually adapt the model while managing GPU memory and training time.\n",
        "\n",
        "The fine-tuning process was divided into the following five phases:\n",
        "\n",
        "### Table of Contents\n",
        "- **Phase 1** ‚Äì Decoder-only training\n",
        "- **Phase 2** ‚Äì Encoder-only training\n",
        "- **Phase 3** ‚Äì Joint encoder-decoder fine-tuning\n",
        "- **Phase 4** ‚Äì Low-Rank Adaptation (LoRA) with quantization\n",
        "- **Phase 5** ‚Äì Final LoRA refinement with longer input/output\n"
      ],
      "metadata": {
        "id": "8Dl6qmQQTCi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 1 - Decoder training\n",
        "\n",
        "In the first phase mBART decoder was trained and encoder freezed (weights remained the same). The model learned to adjust text generation mechanism without interacting with it's ability to comprehend the text. Training the decoder separately allows the model to focus on the linguistic fluency and structure of summaries without compromising the encoder's ability to extract meaningful features from the text\n",
        "\n"
      ],
      "metadata": {
        "id": "YrZnYRUhen7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing packages\n",
        "* -U - upgrade\n",
        "* transformers - models and librarries\n",
        "* huggingface_hub - connects and allows to use or import models to huggingface\n",
        "* datasets - data preparation, import\n",
        "* evaluate - metrics to valuate results\n",
        "* accelerate - optimizing training performance\n",
        "* --force-reinstall --no-cache-dir transformers huggingface_hub - reinstalls the packages and bypassing cache copies (needed for colab environment to prevent huggingface and transformers versions conflict)\n",
        "* rouge_score - installs rogue metrics\n",
        "* bitsandbytes - for low VRAM LoRA training\n",
        "* peft - framework for PEFT method (together with transformers and accelerate due to peft need of specific version)"
      ],
      "metadata": {
        "id": "Oo72c5Tke0b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers huggingface_hub datasets evaluate accelerate\n",
        "!pip install --force-reinstall --no-cache-dir transformers huggingface_hub\n",
        "!pip install rouge_score\n",
        "!pip install -U bitsandbytes peft transformers accelerate\n",
        "!pip install evaluate rouge_score nltk bert_score"
      ],
      "metadata": {
        "id": "eIrGD6GEkZ7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import modules and functions\n",
        "\n",
        "**torch**\n",
        "* torch.cuda.empty_cache() - Frees GPU memory from PyTorch cache (GPU memory optimization (essential for limited resource environments like Colab)\n",
        "* os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] - PyTorch environment configuration to optimize GPU memory management during training\n",
        "* expandable_segments:True - allows for more efficient management of segments in GPU memory\n",
        "* garbage_collection_threshold:0.8 - memory is cleared when 80% GPU load is reached\n",
        "\n",
        "**transformers**\n",
        "* MBartTokenizer - mBART text to token coder\n",
        "* MBartForConditionalGeneration - model architecture for text generation (sequence-to-sequence)\n",
        "* Seq2SeqTrainingArguments - training configuration (batch size, epoch number, optimizer)\n",
        "* Seq2SeqTrainer - sequence-to-sequence trainer with aditional text generate and valuation logic\n",
        "* DataCollatorForSeq2Seq - aligns all texts and summaries to the same length in the batches\n",
        "\n",
        "**peft (Parameter-Efficient Fine-Tuning)**\n",
        "* LoraConfig - allows to set how many layers of weights to apply, tasks and where to use LoRA (overall saves GPU resources)\n",
        "* get_peft_model - combines model with LoRA configuration (saves GPU resources)\n",
        "* TaskType - indicates type of task (needed for LoraConfig for correct LoRA integration)"
      ],
      "metadata": {
        "id": "5Z2OhfW8kf4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,garbage_collection_threshold:0.8\"\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    MBartTokenizer,\n",
        "    MBartForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "import evaluate\n",
        "from huggingface_hub import login\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "LI1NNhtkkf2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data\n",
        "Uploading training, validation and testing data\n",
        "\n",
        "* model_checkpoint - model name from Hugging Face Hub\n",
        "* src_lang=\"lt_LT\" - input in Lithuanian language\n",
        "* tgt_lang=\"lt_LT\"- output in Lithuanian language"
      ],
      "metadata": {
        "id": "UqrL_ZmTkf0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "val_df = pd.read_csv(\"/content/validation.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "    \"test\": Dataset.from_pandas(test_df),\n",
        "})\n",
        "\n",
        "model_checkpoint = \"facebook/mbart-large-50\"\n",
        "tokenizer = MBartTokenizer.from_pretrained(model_checkpoint, src_lang=\"lt_LT\", tgt_lang=\"lt_LT\")"
      ],
      "metadata": {
        "id": "uC08l40KkfyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder training\n",
        "## Inicialize and configure the model\n",
        "* from_pretrained - uploading facebook/mbart-large-50\n",
        "* load_in_4bit - model is loaded in 4 bit (to reduce memory consumption and increase the speed of calculation)\n",
        "* device_map=\"auto\" - automatically switches between GPU/CPU (usefull in environments like Colab to reduce the memmory)\n",
        "* use_cache=False - disables cache (needed for LoRA model)\n",
        "* torch_dtype=torch.bfloat16 - brain float is 16 bit format whitch decreases memory usage without the increase of loss in accuracy.\n",
        "* model.enable_input_require_grads() - allows grad calculation (needed for LoRA model)\n",
        "\n",
        "## Decoder training (freezing encoder)\n",
        "* for name, param in model.model.encoder.named_parameters():\n",
        "  param.requires_grad = freezing encoder\n",
        "\n",
        "## LoRA configuraton\n",
        "* r=128 - LoRA trains only small additional layers instead of the entire model (The larger the r, the more parameters LoRA can learn, but the more memory is required)\n",
        "* lora_alpha=256 - scaling factor that controls how much the LoRA layer's values influence the model's output\n",
        "* target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"] - only these attention layers will be wrapped with LoRA adapters querry, key and value\n",
        "* get_peft_model() - this function \"wraps\" model with LoRA adapters and prepares it to train only the selected components\n",
        "\n",
        "**Overall it enables highly efficient and fast training even on weaker GPUs**\n",
        "\n",
        "## Tokenization\n",
        "* truncation - if text is longer than max_length it will be truncated\n",
        "* padding - adding special PAD tokens to set all the text to the same size (input = 640, output = 90 tokens). If text is too short, PAD will be added.\n",
        "\n",
        "## Training parameters\n",
        "* per_device_train_batch_size=1 - one example per time\n",
        "* gradient_accumulation_steps=16 = 16 steps for eatch example (only then weights are updated)\n",
        "**Setting batch size to 16 (per_device_train_batch_size * gradient_accumulation_steps) while saving GPU RAM\n",
        "* adamw_bnb_8bit - weights and gradients are saved in 8 bit format, but the calclations are done in 32 bites (saves ~70% of the memory)"
      ],
      "metadata": {
        "id": "fAd8zI3Fkfv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MBartForConditionalGeneration.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "for name, param in model.model.encoder.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        "    inference_mode=False,\n",
        "    r=128,\n",
        "    lora_alpha=256,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "def preprocess_phase1(example):\n",
        "    inputs = tokenizer(example[\"text\"], max_length=640, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(example[\"summary\"], max_length=90, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_phase1, batched=True)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"phase1_decoder\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=0.0005,\n",
        "    num_train_epochs=2,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"/content/phase1_decoder\")\n",
        "tokenizer.save_pretrained(\"/content/phase1_decoder\")"
      ],
      "metadata": {
        "id": "NeUoQr2lkfty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phase 2 ‚Äì Encoder-only training\n",
        "In this phase, only the encoder part of the mBART model was fine-tuned while freezing the decoder.This step allows the model to better understand the structure and semantics of the input text before learning how to generate summaries. By training the encoder separately, we ensured that it could produce high-quality internal representations, which later benefit the decoder in downstream summary generation.\n"
      ],
      "metadata": {
        "id": "xHuKsAIpkfrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Difference from Phase 1\n",
        "* model = MBartForConditionalGeneration.from_pretrained(\"/content/phase1_decoder\") - importing model from phase 1\n",
        "* for name, param in model.model.decoder.named_parameters(): param.requires_grad = False - freezing decoder\n",
        "* input max_length = 896\n",
        "* output max_length = 90\n",
        "* lr = 0.0003\n"
      ],
      "metadata": {
        "id": "3X1ak03XkfpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MBartForConditionalGeneration.from_pretrained(\n",
        "    \"/content/phase1_decoder\",\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "for name, param in model.model.decoder.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "def preprocess_phase2(example):\n",
        "    inputs = tokenizer(example[\"text\"], max_length=896, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(example[\"summary\"], max_length=90, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_phase2, batched=True)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"phase2_encoder\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=0.0003,\n",
        "    num_train_epochs=2,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"/content/phase2_encoder\")\n",
        "tokenizer.save_pretrained(\"/content/phase2_encoder\")"
      ],
      "metadata": {
        "id": "j9fNvkgUs4cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Phase 3 ‚Äì Joint Encoder-Decoder Fine-Tuning\n",
        "In this phase, we unfreeze both the encoder and decoder of the mBART model and fine-tune them jointly. This allows the model to learn deeper interactions between input texts and their target summaries. By training the full architecture end-to-end on the task-specific data, we enable it to align encoding and generation more effectively. This step consolidates the gains from previous phases and improves the overall summarization quality."
      ],
      "metadata": {
        "id": "cv3U7E9OvUcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Uploading model\n",
        "* load_in_4bit=True - format reduces VRAM usage\n",
        "* torch_dtype=torch.bfloat16 - format reduces VRAM usage\n",
        "* device_map=\"auto\" - model distribution between devices (GPU, CPU)\n",
        "* use_cache=False - saves memory during training when cache is not needed Reduces VRAM usage (~10-20%).\n",
        "* model.enable_input_require_grads() - forces the model's input layers to calculate gradients even if they would normally be frozen\n",
        "* model = get_peft_model(model, lora_config) - LoRA adapters are added to the model (get_peft_model) so that you can train only a small number of parameters\n",
        "\n",
        "### Training parameters\n",
        "**As previously**\n",
        "* per_device_train_batch_size=1 - one example per time\n",
        "* gradient_accumulation_steps=16 = 16 steps for eatch example (only then weights are updated)\n",
        "**Setting batch size to 16 (per_device_train_batch_size * gradient_accumulation_steps) while saving GPU RAM\n",
        "* predict_with_generate=True - enables text generation when validating the model (validation/test)\n",
        "* fp16=False/bf16=True - bfloat16 is more stable than fp16\n",
        "* adamw_bnb_8bit - weights and gradients are saved in 8 bit format, but the calclations are done in 32 bites (saves ~70% of the memory)\n",
        "*  data_collator=DataCollatorForSeq2Seq(tokenizer, model=model) - automatically padding texts and prepares input for the model"
      ],
      "metadata": {
        "id": "RBFeHN5_vckb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MBartForConditionalGeneration.from_pretrained(\n",
        "    \"/content/phase2_encoder\",\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Same tonenization as in phase 2\n",
        "tokenized_datasets = dataset.map(preprocess_phase2, batched=True)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"phase3_fullmodel\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=0.0001,\n",
        "    num_train_epochs=3,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"/content/phase3_fullmodel\")\n",
        "tokenizer.save_pretrained(\"/content/phase3_fullmodel\")"
      ],
      "metadata": {
        "id": "6U7h6HLswJWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* model.eval() - switching to testing (turning off dropout, batch norm)\n",
        "* predict_with_generate=True - turning text generation on (instead of predict)\n",
        "* do_train=False - model weights remains the same\n",
        "* do_eval=False - since we don't need to use eval in this phase, we are evaluating with test set\n",
        "* skip_special_tokens=True - skipping PAD"
      ],
      "metadata": {
        "id": "Vm3qf1Rq59w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics\n",
        "\n",
        "###ROUGE - Word/sentence matches between generated and label summaries\n",
        "* use_stemmer=True - combines similar word forms\n",
        "\n",
        "###BLEU - N-gram accuracy, how many words/phrases generated by the model match the labels\n",
        "###METEOR - Semantic similarity, including synonyms and word forms\n",
        "###BERTScore - Semantic similarity using BERT embeddings\n",
        "* Precision - How many words in the pattern correspond semantically to the labels\n",
        "* Recall - How many label words to semantically reflect during generation\n",
        "* F1 - Balance between precision and recall"
      ],
      "metadata": {
        "id": "UUKW2h_WO3K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "test_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/eval-output\",\n",
        "    per_device_eval_batch_size=1,\n",
        "    predict_with_generate=True,\n",
        "    do_train=False,\n",
        "    do_eval=False,\n",
        "    fp16=False,\n",
        ")\n",
        "\n",
        "test_trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=test_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        ")\n",
        "\n",
        "test_results = test_trainer.predict(tokenized_datasets[\"test\"])\n",
        "decoded_preds = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(test_results.label_ids, skip_special_tokens=True)\n",
        "\n",
        "df_test_output = pd.DataFrame({\n",
        "    \"Original\": test_df[\"text\"],\n",
        "    \"Reference Summary\": test_df[\"summary\"],\n",
        "    \"Generated Summary\": decoded_preds\n",
        "})\n",
        "csv_path = \"/content/summary_results.csv\"\n",
        "df_test_output.to_csv(csv_path, index=False)\n",
        "print(\"CSV i≈°saugotas:\", csv_path)\n",
        "\n",
        "rouge = load(\"rouge\")\n",
        "rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "print(\"\\n ROUGE metrics:\")\n",
        "for k, v in rouge_scores.items():\n",
        "    print(f\"{k.upper()}: {v:.4f}\")\n",
        "\n",
        "bleu = load(\"bleu\")\n",
        "bleu_score = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_labels])\n",
        "print(\"\\n BLEU SCORE:\")\n",
        "print(f\"BLEU: {bleu_score['bleu']:.4f}\")\n",
        "\n",
        "meteor = load(\"meteor\")\n",
        "meteor_score = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "print(\"\\n METEOR SCORE:\")\n",
        "print(f\"METEOR: {meteor_score['meteor']:.4f}\")\n",
        "\n",
        "bertscore = load(\"bertscore\")\n",
        "bert_results = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"lt\", model_type=\"xlm-roberta-large\")\n",
        "\n",
        "avg_precision = sum(bert_results[\"precision\"]) / len(bert_results[\"precision\"])\n",
        "avg_recall = sum(bert_results[\"recall\"]) / len(bert_results[\"recall\"])\n",
        "avg_f1 = sum(bert_results[\"f1\"]) / len(bert_results[\"f1\"])\n",
        "\n",
        "print(\"\\n BERTScore (xlm-roberta-large):\")\n",
        "print(f\"Precision: {avg_precision:.4f}\")\n",
        "print(f\"Recall:    {avg_recall:.4f}\")\n",
        "print(f\"F1 Score:  {avg_f1:.4f}\")\n",
        "\n",
        "files.download(csv_path)"
      ],
      "metadata": {
        "id": "-RWdSt4OvToV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Phase 3 Results (Joint encoder-decoder training)\n",
        "\n",
        "ROUGE-1 (0.3417): Measures overlap of unigrams (individual words) between generated summaries and references.\n",
        "\n",
        "ROUGE-2 (0.1545): Evaluates overlap of bigrams, indicating slightly deeper linguistic structure.\n",
        "\n",
        "ROUGE-L (0.2614): Considers longest common subsequence, capturing fluency and sentence coherence.\n",
        "\n",
        "BLEU (0.1106):\n",
        "Assesses exact matching of n-grams; a low score here reflects limited word-by-word exact matching but not poor semantic quality.\n",
        "\n",
        "METEOR (0.3032):\n",
        "Evaluates semantic similarity, synonyms, and stemming; reflects moderate semantic accuracy.\n",
        "\n",
        "BERTScore F1 (0.9012):\n",
        "High score indicates very strong semantic similarity with references."
      ],
      "metadata": {
        "id": "YdFiatLYR-sU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Phase 4 ‚Äì Low-Rank Adaptation (LoRA) with quantization\n",
        "In this phase, we apply Low-Rank Adaptation (LoRA) to the fine-tuned mBART model, significantly reducing the number of trainable parameters. LoRA inserts lightweight trainable layers into specific parts of the model, allowing efficient fine-tuning without updating the full model weights.\n",
        "\n",
        "Simultaneously, we apply 4-bit quantization, which reduces memory usage and computational cost by storing weights in lower precision. This combination allows for continued training with limited hardware resources (Google Colab) while maintaining or even improving performance"
      ],
      "metadata": {
        "id": "-0U5jkjxRZQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import modules\n",
        "* import gc - gc.collect() cleans up Pythons memory\n",
        "* os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] - PyTorch environment configuration to optimize GPU memory management during training\n",
        "* expandable_segments:True - allows for more efficient management of segments in GPU memory\n",
        "* garbage_collection_threshold:0.8 - memory is cleared when 80% GPU load is reached"
      ],
      "metadata": {
        "id": "5kgLiwKLZw9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "U7DtiyeUan0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data\n",
        "Uploading training and validation data\n",
        "\n",
        "* model_checkpoint - model name from Hugging Face Hub\n",
        "* src_lang=\"lt_LT\" - input in Lithuanian language\n",
        "* tgt_lang=\"lt_LT\"- output in Lithuanian language\n",
        "\n",
        "**Using fully trained model from phase 3**"
      ],
      "metadata": {
        "id": "cDji2MSIcTMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,garbage_collection_threshold:0.8\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "val_df = pd.read_csv(\"/content/validation.csv\")\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "})\n",
        "\n",
        "model_dir = \"/content/phase3_fullmodel\"\n",
        "tokenizer = MBartTokenizer.from_pretrained(model_dir, src_lang=\"lt_LT\", tgt_lang=\"lt_LT\")"
      ],
      "metadata": {
        "id": "yLmsPCD8cYuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LoRA training\n",
        "###Tokenization\n",
        "* Slightly increased output size to 96 tokens\n",
        "\n",
        "###Uploading LoRA model\n",
        "* config = PeftConfig.from_pretrained(model_dir) - importing peft configurations from previously saved LoRA adapter\n",
        "* base_model = MBartForConditionalGeneration.from_pretrained - importing original mBART model as base for LoRA adapters\n",
        "* config.base_model_name_or_path - original model path\n",
        "* device_map=\"auto\" - distributes the model across the GPU automatically\n",
        "* model = PeftModel.from_pretrained - combines base mBART model with LoRA adapters (from model_dir)\n",
        "* model.enable_input_require_grads() - enables backpropagation of gradients\n",
        "* if \"lora\" in name - cickle through all model parameters, but  backpropagation of gradients enabled only for layers with names \"lora\" (adapters parameters), others remain freezed\n",
        "\n",
        "### Training parameters\n",
        "**As previously**\n",
        "* per_device_train_batch_size=1 - one example per time\n",
        "* gradient_accumulation_steps=16 = 16 steps for eatch example (only then weights are updated)\n",
        "**Setting batch size to 16 (per_device_train_batch_size * gradient_accumulation_steps) while saving GPU RAM\n",
        "* predict_with_generate=True - enables text generation when validating the model (validation/test)\n",
        "* fp16=False/bf16=True - bfloat16 is more stable than fp16\n",
        "* adamw_bnb_8bit - weights and gradients are saved in 8 bit format, but the calclations are done in 32 bites (saves ~70% of the memory)\n",
        "\n",
        "### Trainer\n",
        "* trainer = Seq2SeqTrainer() - combines model, data, tokenizer and evaluation logic\n",
        "* model=model - previously trained mBART model with LoRA adapters\n",
        "* args=training_args - where to save model, number of epochs, batch size, optimizer\n",
        "* train_dataset=tokenized_datasets[\"train\"] - tokenized train dataset\n",
        "* eval_dataset=tokenized_datasets[\"validation\"] - tokenized validation dataset\n",
        "* tokenizer=tokenizer - tokenizing input/output text, calculating eval metrics\n",
        "* data_collator=DataCollatorForSeq2Seq() - assembles batches by properly padding them into tensors of the same size (without it, different length texts would cost errors)"
      ],
      "metadata": {
        "id": "Na-_hInqcqCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"text\"], max_length=896, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=96, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "config = PeftConfig.from_pretrained(model_dir)\n",
        "base_model = MBartForConditionalGeneration.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "model.enable_input_require_grads()\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "login(token=\"hf\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/mbart-lt-phase4\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=0.0002,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=100,\n",
        "    eval_steps=500,\n",
        "    save_steps=1000,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    hub_model_id=\"Arnold001/mbart-lt-summary-phase4\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "save_path = \"/content/mbart-lt-phase4\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "H8xNqVDla5nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation\n",
        "* val_dataset = val_dataset.map(preprocess_function, batched=True) - processes validation data to prepare it for the model input format. Tokenizes texts and summaries and processes data in batches\n",
        "* PeftConfig.from_pretrained(model_dir) - The configuration is loaded from the model_dir directory, which stores information about how LoRA adapters were trained\n",
        "* model = PeftModel.from_pretrained(base_model, model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)  - LoRA adapters from the model_dir directory are added to the base model. The weights of the base model remain frozen ‚Äì only the adapter parameters change\n",
        "\n",
        "**device_map=\"auto\" and torch_dtype=torch.bfloat16: Ensures that adapters are loaded in a format consistent with the base model**\n",
        "\n",
        "* rouge = evaluate.load(\"rouge\") - assesses the quality of summaries\n",
        "* predictions, labels = eval_pred  - model predictions and labels are decoded\n",
        "* use_stemmer=True - compares the roots of words\n",
        "\n",
        "* predict_with_generate=True - the model generates text (summaries) during evaluation\n",
        "* bf16=True - uses 16-bit calculations to save VRAM.\n",
        "* do_train=False - disables the learning process (assessment only)."
      ],
      "metadata": {
        "id": "8g1m1zUYJlk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/mbart-lt-phase4\"\n",
        "\n",
        "val_df = pd.read_csv(\"/content/validation.csv\")\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "tokenizer = MBartTokenizer.from_pretrained(model_dir, src_lang=\"lt_LT\", tgt_lang=\"lt_LT\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"text\"], max_length=896, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=96, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "config = PeftConfig.from_pretrained(model_dir)\n",
        "base_model = MBartForConditionalGeneration.from_pretrained(config.base_model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = PeftModel.from_pretrained(base_model, model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "eval_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/eval-output\",\n",
        "    per_device_eval_batch_size=1,\n",
        "    predict_with_generate=True,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    bf16=True,\n",
        "    logging_dir=None\n",
        ")\n",
        "\n",
        "eval_trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=eval_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "val_results = eval_trainer.evaluate(eval_dataset=val_dataset)\n",
        "print(\"Validation Results:\")\n",
        "print(val_results)"
      ],
      "metadata": {
        "id": "1E64GSHdAotD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test\n",
        "\n",
        "###Metrics\n",
        "\n",
        "###ROUGE - Word/sentence matches between generated and label summaries\n",
        "* use_stemmer=True - combines similar word forms\n",
        "\n",
        "###BLEU - N-gram accuracy, how many words/phrases generated by the model match the labels\n",
        "###METEOR - Semantic similarity, including synonyms and word forms\n",
        "###BERTScore - Semantic similarity using BERT embeddings\n",
        "* Precision - How many words in the pattern correspond semantically to the labels\n",
        "* Recall - How many label words to semantically reflect during generation\n",
        "* F1 - Balance between precision and recall"
      ],
      "metadata": {
        "id": "fvhVWhWDNH3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "test_results = eval_trainer.predict(test_dataset)\n",
        "decoded_preds = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(test_results.label_ids, skip_special_tokens=True)\n",
        "\n",
        "df_test_output = pd.DataFrame({\n",
        "    \"Original\": test_df[\"text\"],\n",
        "    \"Reference Summary\": test_df[\"summary\"],\n",
        "    \"Generated Summary\": decoded_preds\n",
        "})\n",
        "df_test_output.to_csv(\"summary_results.csv\", index=False)\n",
        "print(\"üìÅ Santraukos i≈°saugotos ƒØ summary_results.csv\")\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "print(\"\\n ROUGE metrikos:\")\n",
        "for k, v in rouge_scores.items():\n",
        "    print(f\"{k.upper()}: {v:.4f}\")\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_score = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_labels])\n",
        "print(\"\\n BLEU SCORE:\")\n",
        "print(f\"BLEU: {bleu_score['bleu']:.4f}\")\n",
        "\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "meteor_score = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "print(\"\\n METEOR SCORE:\")\n",
        "print(f\"METEOR: {meteor_score['meteor']:.4f}\")\n",
        "\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "bert_results = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"lt\", model_type=\"xlm-roberta-large\")\n",
        "\n",
        "avg_precision = sum(bert_results[\"precision\"]) / len(bert_results[\"precision\"])\n",
        "avg_recall = sum(bert_results[\"recall\"]) / len(bert_results[\"recall\"])\n",
        "avg_f1 = sum(bert_results[\"f1\"]) / len(bert_results[\"f1\"])\n",
        "\n",
        "print(\"\\n BERTScore:\")\n",
        "print(f\"Precision: {avg_precision:.4f}\")\n",
        "print(f\"Recall:    {avg_recall:.4f}\")\n",
        "print(f\"F1 Score:  {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "-I9-4d5IA1j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Phase 4 Results (LoRA training)\n",
        "\n",
        "ROUGE-1 increased to 0.365 (+0.0233), indicating improved lexical precision.\n",
        "\n",
        "ROUGE-2 increased to 0.1653 (+0.0108), reflecting better phrase-level coherence.\n",
        "\n",
        "ROUGE-L increased to 0.2707 (+0.0093), demonstrating better fluency.\n",
        "\n",
        "BLEU slightly increased to 0.1121 (+0.0015), minimal improvement in exact n-gram matching.\n",
        "\n",
        "METEOR increased to 0.3101 (+0.0069), indicating enhanced semantic precision.\n",
        "\n",
        "BERTScore F1 slightly improved to 0.9025 (+0.0013), maintaining high semantic accuracy.\n",
        "\n",
        "Introducing LoRA allowed more efficient fine-tuning without sacrificing quality. All metrics improved, highlighting LoRA's effectiveness in capturing nuanced semantic and structural details."
      ],
      "metadata": {
        "id": "EjwloEhmSVhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Phase 5 ‚Äì Final LoRA refinement with longer input/output\n",
        "\n",
        "In this final phase, we perform an additional round of LoRA-based fine-tuning, but this time we increase the input and output sequence lengths. This allows the model to better handle longer articles and generate more detailed summaries.  We continue updating only the lightweight LoRA layers‚Äîkeeping the model efficient‚Äîwhile allowing it to learn richer representations from longer texts.\n",
        "\n",
        "This phase maximizes the model's summarization quality within resource constraints, completing the cascaded training process."
      ],
      "metadata": {
        "id": "_l39nTHAsBKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Difference from Phase 4\n",
        "* input max_length increased from 896 to 1024\n",
        "* output max_length increased from 96 to 128"
      ],
      "metadata": {
        "id": "_L-DCGvjtMIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,garbage_collection_threshold:0.8\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "val_df = pd.read_csv(\"/content/validation.csv\")\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "})\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"text\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "model_dir = \"/content/phase4_fullmodel\"\n",
        "config = PeftConfig.from_pretrained(model_dir)\n",
        "\n",
        "base_model = MBartForConditionalGeneration.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "tokenizer = MBartTokenizer.from_pretrained(model_dir, src_lang=\"lt_LT\", tgt_lang=\"lt_LT\")\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "login(token=\"hf\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/mbart-lt-phase5\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=0.0002,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=100,\n",
        "    eval_steps=500,\n",
        "    save_steps=1000,\n",
        "    bf16=True,\n",
        "    fp16=False,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    hub_model_id=\"Arnold001/mbart-lt-summary-phase5\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "save_path = \"/content/mbart-lt-phase5\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "BMH54he9s1jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/mbart-lt-phase5\"\n",
        "\n",
        "val_df = pd.read_csv(\"/content/validation.csv\")\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "tokenizer = MBartTokenizer.from_pretrained(model_dir, src_lang=\"lt_LT\", tgt_lang=\"lt_LT\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"text\"], max_length=896, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=96, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "config = PeftConfig.from_pretrained(model_dir)\n",
        "base_model = MBartForConditionalGeneration.from_pretrained(config.base_model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "model = PeftModel.from_pretrained(base_model, model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "eval_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/eval-output\",\n",
        "    per_device_eval_batch_size=1,\n",
        "    predict_with_generate=True,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    bf16=True,\n",
        "    logging_dir=None\n",
        ")\n",
        "\n",
        "eval_trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=eval_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "val_results = eval_trainer.evaluate(eval_dataset=val_dataset)\n",
        "print(\"Validation Results:\")\n",
        "print(val_results)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WSksx_f3BJyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test\n",
        "\n",
        "###Metrics\n",
        "\n",
        "###ROUGE - Word/sentence matches between generated and label summaries\n",
        "* use_stemmer=True - combines similar word forms\n",
        "\n",
        "###BLEU - N-gram accuracy, how many words/phrases generated by the model match the labels\n",
        "###METEOR - Semantic similarity, including synonyms and word forms\n",
        "###BERTScore - Semantic similarity using BERT embeddings\n",
        "* Precision - How many words in the pattern correspond semantically to the labels\n",
        "* Recall - How many label words to semantically reflect during generation\n",
        "* F1 - Balance between precision and recall"
      ],
      "metadata": {
        "id": "r3kEP4diNox8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "test_results = eval_trainer.predict(test_dataset)\n",
        "decoded_preds = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(test_results.label_ids, skip_special_tokens=True)\n",
        "\n",
        "df_test_output = pd.DataFrame({\n",
        "    \"Original\": test_df[\"text\"],\n",
        "    \"Reference Summary\": test_df[\"summary\"],\n",
        "    \"Generated Summary\": decoded_preds\n",
        "})\n",
        "df_test_output.to_csv(\"summary_results.csv\", index=False)\n",
        "print(\"üìÅ Santraukos i≈°saugotos ƒØ summary_results.csv\")\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "print(\"\\n ROUGE metrikos:\")\n",
        "for k, v in rouge_scores.items():\n",
        "    print(f\"{k.upper()}: {v:.4f}\")\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_score = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_labels])\n",
        "print(\"\\n BLEU SCORE:\")\n",
        "print(f\"BLEU: {bleu_score['bleu']:.4f}\")\n",
        "\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "meteor_score = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "print(\"\\n METEOR SCORE:\")\n",
        "print(f\"METEOR: {meteor_score['meteor']:.4f}\")\n",
        "\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "bert_results = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"lt\", model_type=\"xlm-roberta-large\")\n",
        "\n",
        "avg_precision = sum(bert_results[\"precision\"]) / len(bert_results[\"precision\"])\n",
        "avg_recall = sum(bert_results[\"recall\"]) / len(bert_results[\"recall\"])\n",
        "avg_f1 = sum(bert_results[\"f1\"]) / len(bert_results[\"f1\"])\n",
        "\n",
        "print(\"\\n BERTScore:\")\n",
        "print(f\"Precision: {avg_precision:.4f}\")\n",
        "print(f\"Recall:    {avg_recall:.4f}\")\n",
        "print(f\"F1 Score:  {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "rhDZn0hWBdRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Phase 5 Results (Final LoRA with longer input/output)\n",
        "\n",
        "ROUGE-1 slightly improved to 0.3655 (+0.0005), essentially stable.\n",
        "\n",
        "ROUGE-2 improved to 0.168 (+0.0027), indicating a small enhancement in capturing two-word combinations.\n",
        "\n",
        "ROUGE-L improved to 0.2752 (+0.0045), slight boost in coherence.\n",
        "\n",
        "BLEU slightly decreased to 0.1096 (-0.0025), negligible change and statistically insignificant.\n",
        "\n",
        "METEOR further increased to 0.3116 (+0.0015), reflecting stable semantic gains.\n",
        "\n",
        "BERTScore F1 slightly increased to 0.9027 (+0.0002), sustaining excellent semantic performance.\n",
        "\n",
        "Expanding input and output length allowed the model to utilize richer contextual information, maintaining semantic accuracy and coherence with slight incremental gains across metrics."
      ],
      "metadata": {
        "id": "O42pLYd4SjvQ"
      }
    }
  ]
}
